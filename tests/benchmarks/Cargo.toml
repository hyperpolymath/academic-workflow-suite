[package]
name = "aws-benchmarks"
version = "0.1.0"
edition = "2021"
authors = ["Academic Workflow Suite Team"]
description = "Performance benchmarking suite for AWS"
license = "MIT"

[dependencies]
aws-core = { path = "../../components/core" }
ai-jail = { path = "../../components/ai-jail" }

# Benchmarking
criterion = { version = "0.5", features = ["html_reports", "async_tokio"] }
tokio = { version = "1.35", features = ["full", "test-util"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Database
heed = "0.20"

# Utilities
tempfile = "3.8"
rand = "0.8"
hex = "0.4"

# AI/ML
candle-core = { version = "0.6", features = ["cuda"] }
candle-nn = "0.6"
candle-transformers = "0.6"
tokenizers = { version = "0.19", default-features = false, features = ["onig"] }

# Monitoring
sysinfo = "0.30"

[dev-dependencies]
assert_cmd = "2.0"

[[bench]]
name = "core_benchmarks"
harness = false

[[bench]]
name = "ipc_benchmarks"
harness = false

[[bench]]
name = "ai_benchmarks"
harness = false

[[bench]]
name = "lmdb_bench"
harness = false

[profile.bench]
opt-level = 3
lto = true
codegen-units = 1
